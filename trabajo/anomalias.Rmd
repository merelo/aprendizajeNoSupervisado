---
title: "Trabajo Anomalías"
author: "Miguel Merelo Hernández"
date: "February 18, 2018"
output: pdf_document
---

```{r functions, include=FALSE}
source("../anomalias/!Outliers_A3_Funciones_a_cargar_en_cada_sesion2.R")
source("../anomalias/!Outliers_A2_Librerias_a_cargar_en_cada_sesion.R")
```

# Crear datos
Utilizamos LetterRecognition del paquete mlbench. Solo utilizamos las primeras 200 líneas ya que la visualización de 20000 entradas hace imposible analizar el problema.

```{r, message=F, warning=F, include=FALSE}
library(mlbench)
data("LetterRecognition")
mydata.numeric  = LetterRecognition[1:200,-c(1)] 
indice.columna  = 1
nombre.mydata   = "LetterRecognition"
```

```{r}
mydata.numeric.scaled<-scale(mydata.numeric,center=TRUE,scale=TRUE)
columna<-mydata.numeric[,indice.columna]
nombre.columna<-names(mydata.numeric[indice.columna])
columna.scaled<-scale(columna)
```


# B1. UNIVARIATE STATISTICAL OUTLIERS -> IQR

## 1. Cómputo de los outliers IQR

### Calcular los outliers según la regla IQR. Directamente sin funciones propias.
```{r pressure}
cuartil.primero<-quantile(columna,0.25)
cuartil.tercero<-quantile(columna,0.75)
iqr<-IQR(columna)
iqr
quantile(columna)
```
Obtenemos que el 50% de los datos se encuentran entre los valores 3 y 6, que el valor máximo es 12 y el mínimo es 1. Con IQR=3 no podemos decir que la dispersión de los datos sea muy grande.

```{r}
extremo.superior.outlier.normal  <- cuartil.tercero+1.5*iqr
extremo.inferior.outlier.normal  <- cuartil.primero-1.5*iqr
extremo.superior.outlier.extremo <- cuartil.tercero+3*iqr
extremo.inferior.outlier.extremo <- cuartil.primero-3*iqr

vector.es.outlier.normal <- columna>extremo.superior.outlier.normal | 
  columna<extremo.inferior.outlier.normal
vector.es.outlier.extremo <- columna>extremo.superior.outlier.extremo | 
  columna<extremo.inferior.outlier.extremo


print(paste("Extremo outlier normal.",
            "Inferior:",extremo.inferior.outlier.normal,
            "Superior:",extremo.superior.outlier.normal))
print(paste("Extremo outlier extremo.",
            "Inferior:",extremo.inferior.outlier.extremo,
            "Superior:",extremo.superior.outlier.extremo))
print(paste("Indices de outliers normales:",
            paste(which(vector.es.outlier.normal),collapse = " ")))
print(paste("Indices de outliers extremos:",
            paste(which(vector.es.outlier.extremo), collapse = " ")))
```
Consideramos como outlier normal a aquella entrada con valor menor a -1.5 o superior a 10.5 y como outlier extremo a aquellos con valores menores a -6 y superiores a 15.
Para nuestros datos solo tenemos 2 valores de outliers normales y 0 extremos.
Confirmamos que la dispersión de los datos es pequeña con solo un 1% de outliers.

### Índices y valores de los outliers
```{r}
claves.outliers.normales<-which(vector.es.outlier.normal)
data.frame.outliers.normales<-mydata.numeric[claves.outliers.normales,]
nombres.outliers.normales<-row.names(data.frame.outliers.normales)
valores.outliers.normales<-columna[claves.outliers.normales]
valores.outliers.normales
```
Obtenemos que los dos únicos outliers de la columna que habíamos seleccionado tienen los valores 11 y 12.

### Desviación de los outliers con respecto a la media de la columna.
```{r}
valores.normalizados.outliers.normales<-columna.scaled[vector.es.outlier.normal]
valores.normalizados.outliers.normales
```
Valores de los outliers en el vector normalizado.

### Plot
```{r}
MiPlot_Univariate_Outliers(columna,claves.outliers.normales,nombre.columna)
```








# C1. MULTIVARIATE STATISTICAL OUTLIERS -> Multivariate Normal Distribution -> Mahalanobis

## 1. Obtención de los outliers multivariantes
```{r}
alpha.value = 0.05
alpha.value.penalizado = 1 - ( 1 - alpha.value) ^ (1 / nrow(mydata.numeric))
set.seed(12)  
#solo con las 10 primeras variables, uni.plot no permite mas
mvoutlier.plot<-uni.plot(mydata.numeric[1:10],symb=FALSE,alpha=alpha.value.penalizado)
```
En el gráfico tenemos representados valores escalados de las variables con los que son outliers multivariantes.

## 2. Análisis de los outliers
```{r}
is.MCD.outlier<-mvoutlier.plot$outliers
numero.de.outliers.MCD<-sum(is.MCD.outlier)
numero.de.outliers.MCD
```
En nuestro conjunto de datos tenemos 53 outliers multivariantes.


```{r}
indices.de.outliers.en.alguna.columna<-
  vector_claves_outliers_IQR_en_alguna_columna(mydata.numeric)
indices.de.outliers.en.alguna.columna<-
  indices.de.outliers.en.alguna.columna[!duplicated(indices.de.outliers.en.alguna.columna)]
indices.de.outliers.multivariantes.MCD<-which(is.MCD.outlier)
indices.de.outliers.multivariantes.MCD.pero.no.1variantes<-
  setdiff(indices.de.outliers.multivariantes.MCD,indices.de.outliers.en.alguna.columna)
nombres.de.outliers.multivariantes.MCD.pero.no.1variantes<-
  names(is.MCD.outlier[indices.de.outliers.multivariantes.MCD.pero.no.1variantes])
indices.de.outliers.multivariantes.MCD.pero.no.1variantes
```
Indices de los outliers que son únicamente multivariantes.

```{r}
data.frame.solo.outliers<-mydata.numeric.scaled[is.MCD.outlier,]
head(data.frame.solo.outliers)
```
Valores normalizados de las instancias con outliers.

```{r}
set.seed(12) 
MiBoxPlot_juntos(mydata.numeric[1:10],is.MCD.outlier)
```
Podemos ver como se muestran en rojo las instancias con outliers simples y multivariables.

```{r}
set.seed(12)
MiBiPlot_Multivariate_Outliers(mydata.numeric[1:10],is.MCD.outlier,"LETTERRECOGNITION")
```
En este gráfico se colorean en rojo aquellos valores que son outliers multivariantes. 

```{r}
set.seed(12) 
MiPlot_Univariate_Outliers(mydata.numeric[1:10],
                           indices.de.outliers.en.alguna.columna,
                           "LETTERRECOGNITION")
```
Mostramos las variables dos a dos y marcamos aquellos puntos pertenecientes a instancias que tengan algún outlier en alguna variable. Por la cantidad de instancias con outliers que tenemos, casi el 50%, es imposible sacar una conclusión de este gráfico.







# D1. MULTIVARIATE STATISTICAL OUTLIERS -> LOF 
```{r}
mis.datos.numericos<-LetterRecognition[1:200,-c(1)]
mis.datos.numericos.normalizados<-scale(mis.datos.numericos)
row.names(mis.datos.numericos.normalizados)<-row.names(mis.datos.numericos)
```

```{r}
alpha.value = 0.05
alpha.value.penalizado = 1 - ( 1 - alpha.value) ^ (1 / nrow(mis.datos.numericos))
set.seed(12) 
mvoutlier.plot<-uni.plot(mis.datos.numericos[1:10],
                         symb=FALSE,
                         alpha=alpha.value.penalizado)
```
Tenemos el mismo gráfico ya analizado con los outliers multivariables marcados en rojo.

```{r}
is.MCD.outlier<-mvoutlier.plot$outliers
numero.de.outliers.MCD<-sum(is.MCD.outlier)
corr.plot(mis.datos.numericos[,1], mis.datos.numericos[,3]) 
```
Entre estas dos variables podemos ver como hay dos outliers bastante claros con valores 11 y 12 en X y 13 y 12 en Y.

## 1. DISTANCE BASED OUTLIERS (LOF)
```{r}
numero.de.vecinos.lof = 5
set.seed(12) 
lof.scores<-lofactor(mis.datos.numericos.normalizados,numero.de.vecinos.lof)
plot(lof.scores)
```
Vemos que hay 5 puntos con un lof claramente más alto que los demás, por encima de 1.6 y otros 2 que destacan entre 1.4 y 1.6 por lo que fijaremos el número de outliers en 7.

```{r}
numero.de.outliers = 7
indices.de.lof.outliers.ordenados<-order(lof.scores,decreasing=TRUE)
indices.de.lof.top.outliers<-indices.de.lof.outliers.ordenados[1:numero.de.outliers]
is.lof.outlier<-row.names(mis.datos.numericos) %in% indices.de.lof.top.outliers
indices.de.lof.top.outliers
MiBiPlot_Multivariate_Outliers(mis.datos.numericos,
                               is.lof.outlier,
                               "LETTERRECOGNITION")
```
Los 5 puntos con mayor lof son 10, 28, 52, 31 y 66 mientras que los dos que consideramos outliers por superar el valor 1.4 de lof son 191 y 158.

```{r}
vector.claves.outliers.IQR.en.alguna.columna<-
  vector_claves_outliers_IQR_en_alguna_columna(mis.datos.numericos)
vector.es.outlier.IQR.en.alguna.columna<-vector_es_outlier_IQR_en_alguna_columna(mis.datos.numericos)
MiBiPlot_Multivariate_Outliers(mis.datos.numericos,vector.es.outlier.IQR.en.alguna.columna,"LETTERRECOGNITION")
```
Vemos que los puntos con mayor lof que habíamos seleccionado son outliers por columna.

```{r}
indices.de.outliers.multivariantes.LOF.pero.no.1variantes<-setdiff(vector.claves.outliers.IQR.en.alguna.columna,indices.de.lof.top.outliers)
sort(indices.de.outliers.multivariantes.LOF.pero.no.1variantes)
```
Confirmamos la conclusión del gráfico anterior por lo que podemos decir que, para nuestro caso, usando distancia de Mahalanobis o usando LOF llegamos al mismo resultado.

```{r}
data.frame.numeric<-LetterRecognition[sapply(LetterRecognition,is.numeric)]
head(data.frame.numeric)
```






# D2. MULTIVARIATE STATISTICAL OUTLIERS. CLUSTERING OUTLIERS
Para mejorar la visualización de los datos, solo vamos a considerar las instancias con las 3 primeras letras, A, B y C, de nuestros datos. Por ello fijaremos el número del cluster en 3
```{r, message=F, warning=F}
library(mlbench)
data("LetterRecognition")
mis.datos.numericos<-LetterRecognition[which(LetterRecognition$lettr %in% LETTERS[1:3]),-c(1)]
mis.datos.numericos<-mis.datos.numericos[1:200,]
mis.datos.numericos.normalizados<-scale(mis.datos.numericos)
row.names(mis.datos.numericos.normalizados)<-row.names(mis.datos.numericos)
numero.de.outliers   = 7
numero.de.clusters   = 3

set.seed(2)
```

```{r}
result<-kmeans(mis.datos.numericos.normalizados,numero.de.clusters)
indices.clustering.LetterRecognition<-result$cluster
centroides.normalizados.LetterRecognition<-result$centers
head(indices.clustering.LetterRecognition)
```
Creamos los clusters y se asocia cada instancia a un cluster.

```{r, message=F, warning=F}
distancias_a_centroides = function (datos.normalizados, 
                                    indices.asignacion.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums((datos.normalizados-
                  datos.centroides.normalizados[indices.asignacion.clustering,])^2))
}
```
```{r}
dist.centroides.LetterRecognition<-
  distancias_a_centroides(mis.datos.numericos.normalizados,
                          indices.clustering.LetterRecognition,
                          centroides.normalizados.LetterRecognition)
top.outliers.LetterRecognition<-
  order(dist.centroides.LetterRecognition,decreasing=TRUE)[1:numero.de.outliers]
top.outliers.LetterRecognition
```
Las instancias con mayor distancia al centroide son 52, 21, 127, 119, 96, 4 y 141 y serán las que quizás podríamos cambiar de cluster ya que el objetivo que tenemos es el de minimizar la distancia de cada instancia con su centroide.

```{r}
top_clustering_outliers = function(datos.normalizados,
                                   indices.asignacion.clustering,
                                   datos.centroides.normalizados,
                                   numero.de.outliers){
  dist.centroides<-distancias_a_centroides(datos.normalizados,
                                           indices.asignacion.clustering,
                                           datos.centroides.normalizados)
  respuesta<-list()
  dist.centroides.sorted<-order(dist.centroides, decreasing = TRUE)
  respuesta$indices<-head(dist.centroides.sorted,n=numero.de.outliers)
  respuesta$distancias<-dist.centroides[respuesta$indices]
  respuesta
}
top.outliers.kmeans<-top_clustering_outliers(mis.datos.numericos.normalizados,
                                             indices.clustering.LetterRecognition, 
                                             centroides.normalizados.LetterRecognition, 
                                             numero.de.outliers)
top.outliers.kmeans$indices
top.outliers.kmeans$distancias
```
Obtenemos el mismo resultado que previamente junto a la distancia.

```{r}
numero.de.datos   = nrow(mis.datos.numericos)
is.kmeans.outlier = rep(FALSE, numero.de.datos) 
is.kmeans.outlier[top.outliers.kmeans$indices] = TRUE


BIPLOT.isOutlier             = is.kmeans.outlier
BIPLOT.cluster.colors        = c("black","red","blue")
BIPLOT.asignaciones.clusters = indices.clustering.LetterRecognition
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```
Vemos como se colorean los puntos según al cluster al que pertenezcan y que hay algún punto que está muy cerca de otro cluster, como el que del cluster 2 que vemos cerca del cluster 3.

```{r}
mis.datos.medias<-colMeans(mis.datos.numericos)
mis.datos.desviaciones<-apply(mis.datos.numericos,2,sd)
mis.datos.desviaciones.por.centroides<-
  sweep(centroides.normalizados.LetterRecognition,
        mis.datos.desviaciones,FUN = "*",MARGIN = 2)
centroides.valores<-
  sweep(mis.datos.desviaciones.por.centroides,
        mis.datos.medias,FUN="+",MARGIN=2)
centroides.valores
```
Los valores de los centroides estaban normalizados. Revertimos la operación.

```{r}
library(cluster)
mis.datos.numericos.dist<-dist(mis.datos.numericos.normalizados)
modelo.pam<-pam(mis.datos.numericos.dist,k=numero.de.clusters)
medoides.valores <- mis.datos.numericos[modelo.pam$medoids, ]
medoides.valores.normalizados <- mis.datos.numericos.normalizados[modelo.pam$medoids,]

top.clustering.outliers <-top_clustering_outliers(mis.datos.numericos.normalizados,
                                                  modelo.pam$clustering,
                                                  centroides.valores,
                                                  numero.de.outliers)$indices
top.clustering.outliers
modelo.pam$clustering[top.clustering.outliers]

BIPLOT.asignaciones.clusters = modelo.pam$clustering
is.pam.outlier = rep(FALSE, numero.de.datos) 
is.pam.outlier[top.clustering.outliers] = TRUE
BIPLOT.isOutlier=is.pam.outlier
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```
Utilizando PAM y vemos como queda claramente separado el cluster 2 frente a 1 y 3 pero estos dos clusters se entremezclan en otra zona.

```{r}
top_clustering_outliers_distancia_relativa = function(datos.normalizados, 
                                                      indices.asignacion.clustering, 
                                                      datos.centroides.normalizados, 
                                                      numero.de.outliers){
  
  dist_centroides = distancias_a_centroides (datos.normalizados, 
                                             indices.asignacion.clustering, 
                                             datos.centroides.normalizados)
  
  cluster.ids = unique(indices.asignacion.clustering)
  k           = length(cluster.ids)
  
  distancias.a.centroides.por.cluster=
    sapply(1:k ,function(x) {
      dist_centroides[indices.asignacion.clustering==cluster.ids[x]]})
  
  distancias.medianas.de.cada.cluster=
    sapply(1:k ,function(x) median(dist_centroides[[x]]))
  
  todas.las.distancias.medianas.de.cada.cluster=
    distancias.medianas.de.cada.cluster[indices.asignacion.clustering]
  ratios = dist_centroides/todas.las.distancias.medianas.de.cada.cluster
  
  indices.top.outliers=order(ratios, decreasing=T)[1:numero.de.outliers]
  
  list(distancias = ratios[indices.top.outliers], indices = indices.top.outliers)
}



top.outliers.kmeans.distancia.relativa = 
  top_clustering_outliers_distancia_relativa(mis.datos.numericos.normalizados, 
                                             indices.clustering.LetterRecognition, 
                                             centroides.normalizados.LetterRecognition,
                                             numero.de.outliers)


print("Indices de los top k clustering outliers (k-means, usando distancia relativa)")
top.outliers.kmeans.distancia.relativa$indices 
print("Distancias a sus centroides de los top k clustering outliers (k-means, usando distancia relativa)")
top.outliers.kmeans.distancia.relativa$distancias


is.outlier = rep(FALSE, numero.de.datos) 
is.outlier[top.outliers.kmeans.distancia.relativa$indices] = TRUE
BIPLOT.isOutlier=is.outlier

BIPLOT.cluster.colors        = c("black","red","blue")
BIPLOT.asignaciones.clusters = indices.clustering.LetterRecognition
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```
Vemos como los marcados como outliers en distancia relativa son los que están en los puntos exteriores de cada cluster.


# Conclusiones

El manejo de outliers es una parte muy importante a la hora del preprocesamiento de un conjunto de datos, con un correcto tratado podemos lograr mejores resultados de los que tenemos a priori.

Para trabajar con los valores anómalos de un conjunto de datos, es esencial encontrar qué valores lo son. En este trabajo hemos explorado el tratamiento de valores univariantes, valores anómalos dentro de una única característica, y valores multivariantes, que lo son en más de una característica.

Los outliers univariantes los podemos encontrar de dos tipos, normales y extremos. Los calculamos a través del calculo de los cuartiles y de la distancia intercuartil. Será nuestra tarea como analistas de datos el decidir que realizar con ellos.

Con los multivariantes hemos usado distancia de Mahalanobis, Local Outlier Factor (LOF) y k-means. En nuestro caso, utilizando Mahalanobis y usando LOF hemos llegado a los mismos resultados. En el caso de k-means, hemos visto que al tener una gran dispersión en los datos, muchos outliers univariantes, los cluster ocupaban mucha zona aumentando la distancia de los puntos de la frontera del cluster con el centroide y provocando que algún punto quedara lejos de su cluster y muy pegado a otro. Esto se ha solucionado utilizando medias de distancia con los centroides en vez de distancia euclídea. Esta "solución" provoca que clusters muy cercanos vean sus puntos mezclados.

Como vemos, hemos utilizado distintos métodos para detectar anomalías pero ninguno es la panacea, deberemos aplicar uno u otro dependiendo de la distribución de nuestros datos.